{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b988ab3-4335-468f-a099-a484d2058b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150349147d084b4092fbda46b6ae1929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import motors\n",
    "import time\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "#Creating robot\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "\n",
    "#create widgets for the displaying of the image\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "sidebyside = widgets.HBox([display_color, display_depth],layout=layout) #horizontal \n",
    "display(sidebyside) #display the widget\n",
    "\n",
    "#Convert a NumPy array to JPEG-encoded data for display\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO(\"yolo11l_half.engine\")\n",
    "# model = YOLO(\"yolo11n.engine\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"color_video.avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Set the video codec and save the processed video.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # 'mp4v' codec, suitable for MP4 files\n",
    "width, height = 672,376 #VGA resolution\n",
    "fps = 30\n",
    "color_file = cv2.VideoWriter('color_video_processed.avi', fourcc, fps, (width, height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        t1 = cv2.getTickCount()\n",
    "        # Run YOLO inference on the frame\n",
    "        results = model(frame,verbose=False)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "        color_file.write(annotated_frame)\n",
    "\n",
    "        framed = frame.copy()\n",
    "        #set a confidence threshold to filter out unconfident boxes\n",
    "        #https://docs.ultralytics.com/modes/predict/#boxes\n",
    "        conf_threshold = 0.5\n",
    "        for result in results:\n",
    "            #get the human subject\n",
    "            for i in range (len(result.boxes.cls)):\n",
    "                if(result.boxes.cls[i] == 0):  #human subject\n",
    "                    # print(result.boxes.xywh[i])\n",
    "                    if (result.boxes.conf[i] > conf_threshold): #you\n",
    "                        # print()\n",
    "                        bbox = result.boxes.xyxy[i]\n",
    "                        # print(bbox)\n",
    "                        cv2.rectangle(framed, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
    "                        # cv2.imwrite('human.jpg',color_img)\n",
    "                \n",
    "        scale = 0.3\n",
    "        resized_image = cv2.resize(annotated_frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        resized_image2 = cv2.resize(framed, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        display_color.value = bgr8_to_jpeg(resized_image)\n",
    "        display_depth.value = bgr8_to_jpeg(resized_image2)\n",
    "        \n",
    "        # total_time = (cv2.getTickCount() - t1) / cv2.getTickFrequency()\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "color_file.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fa51077-d26e-4e3e-b0f6-e9d9747b1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 11:35:10 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:35:10 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:35:10 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:35:11 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-05-07 11:35:12 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-05-07 11:35:12 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-05-07 11:35:12 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-05-07 11:35:12 UTC][ZED][INFO] [Init]  Serial Number: S/N 39784002\n",
      "Loading yolo11l_half.engine for TensorRT inference...\n"
     ]
    }
   ],
   "source": [
    "# You will need to load the YOLO model if you skip the first code block.\n",
    "# from ultralytics import YOLO\n",
    "# model = YOLO(\"yolo11l_half.engine\")\n",
    "\n",
    "#Start the camera system\n",
    "import traitlets\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pyzed.sl as sl\n",
    "import math\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "# Define a Camera class that inherits from SingletonConfigurable\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any() # monitor the color_value variable\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "\n",
    "        self.zed = sl.Camera()\n",
    "        # Create a InitParameters object and set configuration parameters\n",
    "        init_params = sl.InitParameters()\n",
    "        init_params.camera_resolution = sl.RESOLUTION.VGA #VGA(672*376), HD720(1280*720), HD1080 (1920*1080) or ...\n",
    "        init_params.depth_mode = sl.DEPTH_MODE.ULTRA  # Use ULTRA depth mode\n",
    "        init_params.coordinate_units = sl.UNIT.MILLIMETER  # Use meter units (for depth measurements)\n",
    "\n",
    "        # Open the camera\n",
    "        status = self.zed.open(init_params)\n",
    "        if status != sl.ERROR_CODE.SUCCESS: #Ensure the camera has opened succesfully\n",
    "            print(\"Camera Open : \"+repr(status)+\". Exit program.\")\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "\n",
    "         # Create and set RuntimeParameters after opening the camera\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "\n",
    "        #flag to control the thread\n",
    "        self.thread_runnning_flag = False\n",
    "\n",
    "        # Get the height and width\n",
    "        camera_info = self.zed.get_camera_information()\n",
    "        self.width = camera_info.camera_configuration.resolution.width\n",
    "        self.height = camera_info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(self.width,self.height,sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "        self.point_cloud = sl.Mat(self.width,self.height,sl.MAT_TYPE.F32_C4, sl.MEM.CPU) \n",
    "\n",
    "    def _capture_frames(self): #For data capturing only\n",
    "\n",
    "        while(self.thread_runnning_flag==True): #continue until the thread_runnning_flag is set to be False\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                \n",
    "                # Retrieve Left image\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                # Retrieve depth map. Depth is aligned on the left image\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "    \n",
    "                self.color_value_BGRA = self.image.get_data()\n",
    "                self.color_value= cv2.cvtColor(self.color_value_BGRA, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_image = np.asanyarray(self.depth.get_data())   \n",
    "                \n",
    "    def start(self): #start the data capture thread\n",
    "        if self.thread_runnning_flag == False: #only process if no thread is running yet\n",
    "            self.thread_runnning_flag=True #flag to control the operation of the _capture_frames function\n",
    "            self.thread = threading.Thread(target=self._capture_frames) #link thread with the function\n",
    "            self.thread.start() #start the thread\n",
    "\n",
    "    def stop(self): #stop the data capture thread\n",
    "        if self.thread_runnning_flag == True:\n",
    "            self.thread_runnning_flag = False #exit the while loop in the _capture_frames\n",
    "            self.thread.join() #wait the exiting of the thread       \n",
    "\n",
    "def bgr8_to_jpeg(value):#convert numpy array to jpeg coded data for displaying \n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "459ca14e-311b-442c-a617-afa2c5e67b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f04fe0b3b24b8a8d45ce6dcf07f43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Person:', options=(('None', None),), value=None), Label(value='Sta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150349147d084b4092fbda46b6ae1929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "display_color = widgets.Image(format='jpeg', width='45%') \n",
    "display_depth = widgets.Image(format='jpeg', width='45%')  \n",
    "layout=widgets.Layout(width='100%')\n",
    "\n",
    "\n",
    "# Variable to store the tracked person\n",
    "tracked_person = None\n",
    "last_seen_time = 0\n",
    "selected_person_bbox = None #new\n",
    "TRACK_TIMEOUT = 1.0  \n",
    "\n",
    "person_dropdown = widgets.Dropdown(\n",
    "    options=[('None', None)],  # Start with None option\n",
    "    description='Select Person:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "tracking_status = widgets.Label(value=\"Status: No person selected\")\n",
    "display(widgets.VBox([person_dropdown, tracking_status]))\n",
    "display(sidebyside)\n",
    "\n",
    "def func(change):\n",
    "    global tracked_person\n",
    "    \n",
    "    frame = change['new']\n",
    "    results = model(frame, verbose=False)\n",
    "    \n",
    "    # Get current persons in frame\n",
    "    persons = []\n",
    "    dropdown_options = [('None', None)]  # Always include None option\n",
    "    \n",
    "    for i in range(len(results[0].boxes.cls)):\n",
    "        if results[0].boxes.cls[i] == 0 and results[0].boxes.conf[i] > 0.5:\n",
    "            bbox = results[0].boxes.xyxy[i].cpu().numpy()\n",
    "            persons.append(bbox)\n",
    "            dropdown_options.append((f\"Person {len(dropdown_options)}\", bbox))\n",
    "    \n",
    "    # Update dropdown options if they've changed\n",
    "    if dropdown_options != person_dropdown.options:\n",
    "        with person_dropdown.hold_trait_notifications():\n",
    "            person_dropdown.options = dropdown_options\n",
    "    \n",
    "    # Only track if a person is selected\n",
    "    if person_dropdown.value is not None:\n",
    "        # Find the selected person in current frame\n",
    "        selected_person = None\n",
    "        for bbox in persons:\n",
    "            if np.array_equal(bbox, person_dropdown.value):\n",
    "                selected_person = bbox\n",
    "                break\n",
    "        \n",
    "        if selected_person is not None:\n",
    "            tracked_person = selected_person\n",
    "            distance = getPersonDepth(tracked_person, camera.depth_image)\n",
    "            turn, move = generate_robot_commands(tracked_person, frame.shape[1], distance)\n",
    "            execute_movement(turn, move)\n",
    "            \n",
    "            tracking_status.value = f\"Status: Tracking selected person | Dist: {distance:.0f}mm\"\n",
    "            \n",
    "            # Draw all persons\n",
    "            for i, bbox in enumerate(persons):\n",
    "                color = (0, 255, 0) if np.array_equal(bbox, tracked_person) else (255, 0, 0)\n",
    "                cv2.rectangle(frame, \n",
    "                             (int(bbox[0]), int(bbox[1])),\n",
    "                             (int(bbox[2]), int(bbox[3])),\n",
    "                             color, 2)\n",
    "                if not np.array_equal(bbox, tracked_person):\n",
    "                    cv2.putText(frame, f\"Person {i+1}\", \n",
    "                              (int(bbox[0]), int(bbox[1]-10)),\n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "            \n",
    "            status = f\"Tracking: {move} {turn}\"\n",
    "            cv2.putText(frame, status, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            robot.stop()\n",
    "            tracking_status.value = \"Status: Selected person not found in frame\"\n",
    "    else:\n",
    "        robot.stop()\n",
    "        tracking_status.value = \"Status: No person selected\"\n",
    "        \n",
    "        # Draw all persons (but don't track)\n",
    "        for i, bbox in enumerate(persons):\n",
    "            cv2.rectangle(frame, \n",
    "                         (int(bbox[0]), int(bbox[1])),\n",
    "                         (int(bbox[2]), int(bbox[3])),\n",
    "                         (255, 0, 0), 2)\n",
    "            cv2.putText(frame, f\"Person {i+1}\", \n",
    "                      (int(bbox[0]), int(bbox[1]-10)),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    \n",
    "    # Display\n",
    "    resized_image = cv2.resize(frame, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "    display_color.value = bgr8_to_jpeg(resized_image)\n",
    "    \n",
    "    depth_colormap = cv2.applyColorMap(\n",
    "        cv2.convertScaleAbs(camera.depth_image, alpha=0.03), cv2.COLORMAP_JET\n",
    "    )\n",
    "    resized_depth = cv2.resize(depth_colormap, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "    display_depth.value = bgr8_to_jpeg(resized_depth)\n",
    "\n",
    "\n",
    "\n",
    "camera.observe(func, names=['color_value'])\n",
    "# camera.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0604db-5ca3-415c-8079-51c95a0a75a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be0e5b1-279d-4dc1-9a3e-927d11e5eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_movement(turn, move):\n",
    "    # Movement\n",
    "    if move == 'forward':\n",
    "        robot.forward(0.7) \n",
    "    elif move == 'backward':\n",
    "        robot.backward(0.7)  \n",
    "    else:\n",
    "        robot.stop()\n",
    "    \n",
    "    if turn == 'left':\n",
    "        robot.left(0.4)\n",
    "    elif turn == 'right':\n",
    "        robot.right(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dcdca9-9b54-48ee-b47c-966e0b07799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personSelection(results, image_center, conf_threshold = 0.5):\n",
    "    minimum_distance = float('inf')\n",
    "    selected_bbox = None\n",
    "    \n",
    "    for i in range(len(results[0].boxes.cls)):\n",
    "        if results[0].boxes.cls[i] == 0 and results[0].boxes.conf[i] > conf_threshold:\n",
    "            bbox = results[0].boxes.xyxy[i]\n",
    "            center = (bbox[0]+bbox[2]) / 2\n",
    "            distance = abs(center - image_center)\n",
    "            if distance < minimum_distance:\n",
    "                minimum_distance = distance\n",
    "                selected_bbox = bbox\n",
    "    return selected_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e282864-5112-4856-860a-c050cb86b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPersonDepth(bbox, depth_image):\n",
    "    centerX = int((bbox[0] + bbox[2]) / 2)\n",
    "    centerY = int((bbox[1] + bbox[3]) / 2)\n",
    "    depth = depth_image[centerY, centerX]\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9caa17ad-1d4f-413b-a7be-116d13884aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/07/2025-12:35:13] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[05/07/2025-12:35:13] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[05/07/2025-12:35:13] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n"
     ]
    }
   ],
   "source": [
    "def generate_robot_commands(bbox, frame_width, person_distance, desired_distance=1000):\n",
    "    center_x = (bbox[0] + bbox[2]) / 2\n",
    "    error_x = center_x - (frame_width / 2)\n",
    "    \n",
    "    norm_distance_error = abs(person_distance - desired_distance) / desired_distance\n",
    "    norm_position_error = abs(error_x) / (frame_width / 2)\n",
    "    \n",
    "    distance_dominant_threshold = 0.3  \n",
    "    position_dominant_threshold = 0.2  \n",
    "    \n",
    "\n",
    "    if norm_distance_error > distance_dominant_threshold:\n",
    "        if person_distance < desired_distance - 200:  \n",
    "            move = 'backward'\n",
    "            if person_distance > 500:  \n",
    "                if norm_position_error > 0.3:\n",
    "                    turn = 'right' if error_x > 0 else 'left'\n",
    "                else:\n",
    "                    turn = 'none'\n",
    "            else:\n",
    "                turn = 'none'  \n",
    "        else:\n",
    "            if norm_position_error > position_dominant_threshold:\n",
    "                turn = 'right' if error_x > 0 else 'left'\n",
    "            else:\n",
    "                turn = 'none'\n",
    "            move = 'forward' if person_distance > desired_distance + 200 else 'stop'\n",
    "    else:\n",
    "        if norm_position_error > position_dominant_threshold:\n",
    "            turn = 'right' if error_x > 0 else 'left'\n",
    "        else:\n",
    "            turn = 'none'\n",
    "        move = 'forward' if person_distance > desired_distance + 200 else \\\n",
    "              'backward' if person_distance < desired_distance - 200 else 'stop'\n",
    "    \n",
    "    # When moving backward, reduce turning sensitivity\n",
    "    if move == 'backward':\n",
    "        if norm_position_error > 0.4:  # Higher threshold when backing up\n",
    "            turn = 'right' if error_x > 0 else 'left'\n",
    "        else:\n",
    "            turn = 'none'\n",
    "    \n",
    "    return turn, move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec9cdd-702c-4cd1-b99d-bfbad8993388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
