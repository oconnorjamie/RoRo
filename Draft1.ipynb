{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f092effb-c6e2-4038-9706-b985c93b6d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5cc0b98354433a897ac295b8f252ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', width='45%'), Image(value=b'', format='jpeg', width='45%')), la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1446eb209da5427993139f94ddc29946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Select Person:', options=(('None', None),), value=None), Label(value='Sta…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 11:59:05 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:59:05 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:59:05 UTC][ZED][INFO] Logging level INFO\n",
      "[2025-05-07 11:59:06 UTC][ZED][INFO] [Init]  Depth mode: ULTRA\n",
      "[2025-05-07 11:59:06 UTC][ZED][INFO] [Init]  Camera successfully opened.\n",
      "[2025-05-07 11:59:06 UTC][ZED][INFO] [Init]  Camera FW version: 1523\n",
      "[2025-05-07 11:59:06 UTC][ZED][INFO] [Init]  Video mode: VGA@100\n",
      "[2025-05-07 11:59:06 UTC][ZED][INFO] [Init]  Serial Number: S/N 39784002\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import motors\n",
    "import time\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import traitlets\n",
    "import pyzed.sl as sl\n",
    "import threading\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "robot = motors.MotorsYukon(mecanum=False)\n",
    "model = YOLO(\"yolo11l_half.engine\", task='detect')\n",
    "\n",
    "display_color = widgets.Image(format='jpeg', width='45%')\n",
    "display_depth = widgets.Image(format='jpeg', width='45%')\n",
    "layout = widgets.Layout(width='100%')\n",
    "sidebyside = widgets.HBox([display_color, display_depth], layout=layout)\n",
    "display(sidebyside)\n",
    "\n",
    "def bgr8_to_jpeg(value):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "person_dropdown = widgets.Dropdown(options=[('None', None)], description='Select Person:')\n",
    "tracking_status = widgets.Label(value=\"Status: No person selected\")\n",
    "display(widgets.VBox([person_dropdown, tracking_status]))\n",
    "\n",
    "tracked_person_idx = None\n",
    "def on_person_select(change):\n",
    "    global tracked_person_idx\n",
    "    tracked_person_idx = change['new']\n",
    "person_dropdown.observe(on_person_select, names='value')\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    color_value = traitlets.Any()\n",
    "    depth_value = traitlets.Any()\n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        self.zed = sl.Camera()\n",
    "        p = sl.InitParameters()\n",
    "        p.camera_resolution = sl.RESOLUTION.VGA\n",
    "        p.depth_mode = sl.DEPTH_MODE.ULTRA\n",
    "        p.coordinate_units = sl.UNIT.MILLIMETER\n",
    "        if self.zed.open(p) != sl.ERROR_CODE.SUCCESS:\n",
    "            self.zed.close()\n",
    "            exit(1)\n",
    "        self.runtime = sl.RuntimeParameters()\n",
    "        self.thread_running_flag = False\n",
    "        info = self.zed.get_camera_information()\n",
    "        w = info.camera_configuration.resolution.width\n",
    "        h = info.camera_configuration.resolution.height\n",
    "        self.image = sl.Mat(w, h, sl.MAT_TYPE.U8_C4, sl.MEM.CPU)\n",
    "        self.depth = sl.Mat(w, h, sl.MAT_TYPE.F32_C1, sl.MEM.CPU)\n",
    "    def _capture_frames(self):\n",
    "        while self.thread_running_flag:\n",
    "            if self.zed.grab(self.runtime) == sl.ERROR_CODE.SUCCESS:\n",
    "                self.zed.retrieve_image(self.image, sl.VIEW.LEFT)\n",
    "                self.zed.retrieve_measure(self.depth, sl.MEASURE.DEPTH)\n",
    "                bgra = self.image.get_data()\n",
    "                self.color_value = cv2.cvtColor(bgra, cv2.COLOR_BGRA2BGR)\n",
    "                self.depth_value = np.asanyarray(self.depth.get_data())\n",
    "    def start(self):\n",
    "        if not self.thread_running_flag:\n",
    "            self.thread_running_flag = True\n",
    "            threading.Thread(target=self._capture_frames, daemon=True).start()\n",
    "    def stop(self):\n",
    "        self.thread_running_flag = False\n",
    "\n",
    "camera = Camera()\n",
    "camera.start()\n",
    "\n",
    "def execute_movement(turn, move):\n",
    "    if move=='forward': robot.forward(0.7)\n",
    "    elif move=='backward': robot.backward(0.7)\n",
    "    else: robot.stop()\n",
    "    if turn=='left': robot.left(0.4)\n",
    "    elif turn=='right': robot.right(0.4)\n",
    "\n",
    "def getPersonDepth(bbox, depth_image):\n",
    "    cx = int((bbox[0]+bbox[2])/2)\n",
    "    cy = int((bbox[1]+bbox[3])/2)\n",
    "    d = depth_image[cy,cx]\n",
    "    return d if not (np.isnan(d) or np.isinf(d)) else 0\n",
    "\n",
    "def generate_robot_commands(bbox, fw, dist, desired_distance=1000):\n",
    "    cx = (bbox[0]+bbox[2])/2\n",
    "    ex = cx - fw/2\n",
    "    if dist < desired_distance-200: move='backward'\n",
    "    elif dist > desired_distance+200: move='forward'\n",
    "    else: move='stop'\n",
    "    turn = 'right' if ex>fw*0.1 else 'left' if ex< -fw*0.1 else 'none'\n",
    "    return turn, move\n",
    "\n",
    "def process_frame(frame, depth_frame):\n",
    "    r = model(frame, verbose=False)[0]\n",
    "    persons = [r.boxes.xyxy[i].cpu().numpy()\n",
    "               for i in range(len(r.boxes.cls))\n",
    "               if r.boxes.cls[i]==0 and r.boxes.conf[i]>0.5]\n",
    "    opts = [('None', None)] + [(f\"Person {i+1}\", i) for i in range(len(persons))]\n",
    "    if opts != person_dropdown.options:\n",
    "        person_dropdown.options = opts\n",
    "    if tracked_person_idx is not None and tracked_person_idx < len(persons):\n",
    "        b = persons[tracked_person_idx]\n",
    "        dist = getPersonDepth(b, depth_frame)\n",
    "        turn, move = generate_robot_commands(b, frame.shape[1], dist)\n",
    "        execute_movement(turn, move)\n",
    "        tracking_status.value = f\"Status: Tracking Person {tracked_person_idx+1} | Dist: {dist:.0f} mm\"\n",
    "        for i, bb in enumerate(persons):\n",
    "            c = (0,255,0) if i==tracked_person_idx else (255,0,0)\n",
    "            cv2.rectangle(frame, (int(bb[0]),int(bb[1])), (int(bb[2]),int(bb[3])), c, 2)\n",
    "            if i!=tracked_person_idx:\n",
    "                cv2.putText(frame, f\"Person {i+1}\", (int(bb[0]),int(bb[1]-10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),1)\n",
    "        cv2.putText(frame, f\"{move} {turn}\", (10,30), cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,255,0),2)\n",
    "    else:\n",
    "        robot.stop()\n",
    "        tracking_status.value = (\"Status: No person selected\"\n",
    "                                 if tracked_person_idx is None\n",
    "                                 else \"Status: Selected person not found\")\n",
    "        for i, bb in enumerate(persons):\n",
    "            cv2.rectangle(frame, (int(bb[0]),int(bb[1])), (int(bb[2]),int(bb[3])), (255,0,0),2)\n",
    "            cv2.putText(frame,f\"Person {i+1}\",(int(bb[0]),int(bb[1]-10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,0,0),1)\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c963c57-b923-4889-9bb6-6311f4fc54c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading yolo11l_half.engine for TensorRT inference...\n",
      "[05/07/2025-12:59:38] [TRT] [I] Loaded engine size: 52 MiB\n",
      "[05/07/2025-12:59:38] [TRT] [W] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "[05/07/2025-12:59:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +36, now: CPU 1, GPU 84 (MiB)\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "running = True\n",
    "def run_loop():\n",
    "    while running:\n",
    "        if camera.color_value is not None and camera.depth_value is not None:\n",
    "            f = camera.color_value.copy()\n",
    "            d = camera.depth_value.copy()\n",
    "            pf = process_frame(f, d)\n",
    "            rc = cv2.resize(pf, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "            display_color.value = bgr8_to_jpeg(rc)\n",
    "            dm = cv2.applyColorMap(cv2.convertScaleAbs(d, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            rd = cv2.resize(dm, None, fx=0.3, fy=0.3, interpolation=cv2.INTER_AREA)\n",
    "            display_depth.value = bgr8_to_jpeg(rd)\n",
    "        time.sleep(0.05)\n",
    "\n",
    "threading.Thread(target=run_loop, daemon=True).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52a4246-f05e-4baa-be93-52a753fd9024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
